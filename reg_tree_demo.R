### Running a DEMO of rpart on a continuous outcome variable
# Available at: https://github.com/danielemelotti/decision_tree_exploration

## Installing the necessary packages:
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
require(rpart)
require(rpart.plot)
require(rattle)

# For the purpose of this exercise, we'll be using a dataset containing car purchasing data.
# The dataset can be downloaded from: https://www.kaggle.com/datasets/yashk07/car-purchase-price-beginner-dataset
# The 9 variables included in the dataset are Customer Name, Customer e-mail, Country, Gender,
# Age, Annual Salary, Credit Card Debt, Net Worth, and Car Purchase Amount.

## Loading the data
cardata <- read.csv("Car_Purchasing_Data.csv")
str(cardata)

table(is.na(cardata))
table(cardata$Country)

# There is no missing value. Moreover, variables like Customer Name and Customer email
# will not be considered as they are not determining for the outcome of our analysis.
# Lastly, the Country variable will not be considered either, because all of the customers are from
# the same country, USA.

## Splitting the data with a 75:25 split (split-sample cross-validation)
set.seed(20221028)
train_indices <- sample(1:nrow(cardata), size = 0.75 * nrow(cardata))

train_set <- cardata[train_indices, ]

## Estimating an ols model on the train dataset
model <- Car.Purchase.Amount ~ factor(Gender) + Age + Annual.Salary + 
                               Credit.Card.Debt + Net.Worth
ols <- lm(model, data = train_set)
summary(ols)

## Building and visualizing the tree
tree <- rpart(model, data = train_set)
rpart.plot(tree, type = 2)

# COMMENT: The top value is the average Car.Purchase.Amount for the people represented at a 
# certain node. For example, the root node shows that in the whole train data, the average
# cost for a new car is 45,000; the root's left child node instead presents an average cost
# of 38,000 (for people who have an annual salary lower than 58,000).

# In classification trees there is one more value in each node, just between the two
# just described values. That one is the probability. See https://www.guru99.com/r-decision-trees.html 
# for an extensive example of a classification tree.

# Understanding pruning with printcp()
printcp(tree)

## printcp() PARAMETERS DESCRIPTION
# https://maths-people.anu.edu.au/~johnm/courses/dm/math3346/2006/pdf/r-exs9.pdf
# The parameters included in the table are the following:
# - CP: Complexity Parameter. A smaller cp gives a more complex model, i.e., more splits.
# - nsplit: Literally it is the number of splits. A tree with nsplit = 0 is just a single leaf.
#           A tree with nsplit equal to 1 will have 2 leaves. A tree with nsplit = 2 will have 3 
#           leaves and so on. A leaf is a node that has no child nodes (terminal node).
# - rel error: This is the error on the observations used to estimate the model. Multiply by baseline 
#              error to get the corresponding absolute error measure.
# - xerror: Cross validation error estimate. It is generated by the rpart built-in cross validation.
# - xstd: Standard deviation of the error.

## printcp() PARAMETERS CALCULATION
# - CP: CP_n is obtained by (rel error_n - rel error_(n+1))/(nsplit_(n+1)-nsplit_n) (https://stats.stackexchange.com/questions/117908/rpart-complexity-parameter-confusion).
#   e.g., for nsplit 10:
#     (tree_cp[10, "rel error"] - tree_cp[11, "rel error"]) / (tree_cp[11, "nsplit"] - tree_cp[10, "nsplit"])
# - rel error: 1 - R^2, similar to linear regression (https://stats.stackexchange.com/questions/103018/difference-between-rel-error-and-xerror-in-rpart-regression-trees).
# - xerror: It is computed using a 10-fold cross-validation.
# - xstd: 

# As we can see, printcp() automatically prints out the optimal number of nodes basing on
# the CP (Complexity Parameter). rpart does not always deliver a pruned tree (see: 
# https://stackoverflow.com/questions/13136683/is-rpart-automatic-pruning#:~:text=No%2C%20but%20the%20defaults%20for,definition%20of%20%22early%22).)
# Sometimes, we need to prune it on our own by considering the xerror and cp. The convention says that we 
# should select the CP from the list with the smallest cross-validated error (xerror in the table).
### IN OTHER WORDS, we shall select the lowest xerror, then prune the tree by setting the CP equal to
# the CP corresponding to such error.

# Here is a way to find our the cp related to the lowest xerror without looking through the table
c_par <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
c_par

# Above we have the optimal CP value associated with the mininum error.

## Pruning and visualizing the new tree: let's prune using the just found cp
prune_tree <- prune(tree, cp = c_par)
rpart.plot(prune_tree)

# We can also use printcp() to verify the differences:
printcp(tree)
printcp(prune_tree)

# We see basically that we have removed the very last split only by executing our own pruning Now the number of
# leaves is 14, one less than before.

# Example with fancyRpartPlot
fancyRpartPlot(prune_tree)

# The number on the top of each node represents the branch numbers in the textual representation
# of the trees as generated by the default print() method. See:
# https://stackoverflow.com/questions/45570298/what-does-the-number-on-top-of-a-node-in-a-fancyrpartplot-decision-tree-mean.

## Using plotcp() to show the set of possible cost-complexity prunings of a tree from a nested set.
# https://rdrr.io/cran/itree/man/plotcp.html
# We can compare the tree before vs after pruning
plotcp(tree)
plotcp(prune_tree)

# What is the dotted line in plotcp()?
# https://stackoverflow.com/questions/21698540/whats-the-meaning-of-plotcp-result-in-rpart
# The function plots the cp values against their related xerror, with xstd included.
# The dotted line represents the highest cross-validated error less than the minimum cross-validated
# error plus 1 standard deviation of the error at that tree.

## Predicting the outcome variable on the test set
test_set <- cardata[-train_indices, ]
purchase_predicted_ols <- predict(ols, test_set)
purchase_predicted_tree <- predict(tree, test_set)
purchase_predicted_prune <- predict(prune_tree, test_set)

## Reporting prediction accuracy using the RMSE
# In-sample trained model RMSE, comparison of ols vs Pruned Tree:
rmse_is_ols <- round(sqrt(mean(residuals(ols)^2)), 4)
rmse_is_tree <- round(sqrt(mean(residuals(tree)^2)), 4)
rmse_is_prune <- round(sqrt(mean(residuals(prune_tree)^2)), 4)
rmse_is_ols
rmse_is_tree
rmse_is_prune

# Creating a function to calculate the RMSE out of sample
rmse_oos <- function(actuals, preds) {
  sqrt(mean((preds - actuals)^2))
}

# Out-of-sample test data RMSE:
rmse_oos_ols <- rmse_oos(purchase_predicted_ols, test_set$Car.Purchase.Amount)
rmse_oos_tree <- rmse_oos(purchase_predicted_tree, test_set$Car.Purchase.Amount)
rmse_oos_prune <- rmse_oos(purchase_predicted_prune, test_set$Car.Purchase.Amount)
rmse_oos_ols
rmse_oos_tree
rmse_oos_prune

# For the pruned tree, the RMSE_oos is greater than RMSE_is, as it should be. For the ols, that's not the case.

## Implementing LOOCV
fold_i_pe <- function(i, k, model, dataset, outcome) {
  folds <- cut(1:nrow(dataset), breaks=k, labels=FALSE)
  test_indices <- which(folds==i)
  test_set <- dataset[test_indices, ]
  train_set <- dataset[-test_indices, ]
  trained_model <- update(model, data = train_set)
  predictions <- predict(trained_model, test_set)
  dataset[test_indices, outcome] - predictions # predictive error
}

k_fold_rmse <- function(model, dataset, outcome, k=10) {
  shuffled_indicies <- sample(1:nrow(dataset))
  dataset <- dataset[shuffled_indicies,]

  fold_pred_errors <- sapply(1:k, \(kth) {
    fold_i_pe(kth, k, model, dataset, outcome)
  })
  
  pred_errors <- unlist(fold_pred_errors)
  rmse <- \(errs) sqrt(mean(errs^2))
  c(rmse_is = rmse(residuals(model)), rmse_oos = rmse(pred_errors))
}

k_fold_rmse_ols <- k_fold_rmse(ols, cardata, "Car.Purchase.Amount", k = nrow(cardata))
k_fold_rmse_prune <- k_fold_rmse(prune_tree, cardata, "Car.Purchase.Amount", k = nrow(cardata))

k_fold_rmse_ols
k_fold_rmse_prune

# We can see that when we use LOOCV the RMSE_oos is greater than RMSE_is for both the OLS and pruned tree.

## BAGGING
# Creating the functions for learning and predicting
bagged_learn <- function(model, dataset, b=100) {
  lapply(1:b, \(i) {
    data_i <- dataset[sample(nrow(dataset), replace=TRUE),]
    update(model, data=data_i) 
  })
}

bagged_predict <- function(bagged_models, new_data) {
  predictions <- lapply(bagged_models, \(m) predict(m, new_data))
  as.data.frame(predictions) |> apply(FUN=mean, MARGIN=1)
}

# Computing the RMSEs between ols and prune_tree bagged models
rmse_bag_ols <- bagged_learn(model = ols, dataset = train_set) |>
  bagged_predict(new_data = test_set) |> 
  rmse_oos(actuals = test_set$Car.Purchase.Amount)

rmse_bag_prune <- bagged_learn(model = prune_tree, dataset = train_set) |>
  bagged_predict(new_data = test_set) |> 
  rmse_oos(actuals = test_set$Car.Purchase.Amount)

rmse_bag_ols
rmse_bag_prune

### BOOSTING
# Creating the functions for learning and predicting
boost_learn <- function(model, dataset, outcome, n=100, rate=0.1) { 
  predictors <- dataset[,-which(names(dataset) == outcome)]
  
  res <- dataset[,outcome]
  models <- list()
  
  for (i in 1:n) {
    new_data <- cbind(res, predictors)
    colnames(new_data)[1] = outcome
    this_model <- update(model, data = new_data)
    res <- res - rate * predict(this_model, dataset)
    models[[i]] <- this_model
  }
  
  list(models=models, rate=rate)
}
  
boost_predict <- function(boosted_learning, new_data) {
  boosted_models <- boosted_learning$models
  rate <- boosted_learning$rate
  predictions <- lapply(boosted_models, \(this_model) {
    predict(this_model, new_data)
  })
  pred_frame <- as.data.frame(predictions) |> unname()
  apply(pred_frame, FUN = \(preds) rate * sum(preds), MARGIN=1)
}
  
# Comparing the RMSEs between ols and prune_tree boosted models
rmse_boost_ols <- boost_learn(ols, train_set, "Car.Purchase.Amount") |>
  boost_predict(test_set) |> rmse_oos(actuals = test_set$Car.Purchase.Amount)

rmse_boost_prune <- boost_learn(prune_tree, train_set, "Car.Purchase.Amount") |>
  boost_predict(test_set) |> rmse_oos(actuals = test_set$Car.Purchase.Amount)

rmse_boost_ols
rmse_boost_prune

# Final comparisons
error_comparison <- matrix(c(rmse_is_ols, rmse_oos_ols, rmse_is_prune, rmse_oos_prune, k_fold_rmse_ols[1], 
                       k_fold_rmse_ols[2], k_fold_rmse_prune[1], k_fold_rmse_prune[2]), ncol = 4, byrow = FALSE)

colnames(error_comparison) <- c("rmse_ols", "rmse_prune", "k_fold_rmse_ols", "k_fold_rmse_prune")
rownames(error_comparison) <- c("is", "oos")

error_comparison

bag_boost_comparison <- matrix(c(rmse_bag_ols, rmse_boost_ols, rmse_bag_prune, rmse_boost_prune), 
                               ncol = 2, byrow = FALSE)

colnames(bag_boost_comparison) <- c("rmse_ols", "rmse_prune")
rownames(bag_boost_comparison) <- c("bag", "boost")

bag_boost_comparison