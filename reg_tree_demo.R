### Running a DEMO of rpart on a continuous outcome variable
# Available at: https://github.com/danielemelotti/decision_tree_exploration

## Installing the necessary packages:
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
require(rpart)
require(rpart.plot)
require(rattle)
require(dplyr)

# For the purpose of this exercise, we'll be using a dataset regarding housing prices in Miami.
# The dataset can be downloaded from: https://www.kaggle.com/datasets/deepcontractor/miami-housing-dataset
# The dataset includes a total of 17 variables.

## Loading specific data and model_formula
DL_data <- read.csv("miami-housing.csv")
str(DL_data)

# Filtering for the variables of interest
fulldata <- DL_data %>%
  select(-c("LATITUDE","LONGITUDE", "PARCELNO", "month_sold"))

str(fulldata)

dv <- "SALE_PRC" # outcome variable
model_formula <- SALE_PRC ~ LND_SQFOOT + TOT_LVG_AREA + SPEC_FEAT_VAL + RAIL_DIST + OCEAN_DIST +
  WATER_DIST + CNTR_DIST + SUBCNTR_DI + HWY_DIST + age + avno60plus + structure_quality

table(is.na(fulldata))

# There are no missing values

# Plotting the data to inspect relationships
plot(fulldata[, c(dv, "TOT_LVG_AREA", "RAIL_DIST", "CNTR_DIST", "age", "structure_quality")], 
     col=rgb(0.7, 0.7, 0.7, 0.3)) # inspecting the relationships between the data

## Split-sample cross validation 
# Performing a 75:25 split
set.seed(2012)
train_indices <- sample(1:nrow(fulldata), size = 0.75 * nrow(fulldata))

# Creating the train and test sets
train_set <- fulldata[train_indices, ]
test_set <- fulldata[-train_indices, ]

## Estimating an ols model_formula on the train dataset
ols <- lm(model_formula, data = train_set)
summary(ols)

## Building and visualizing the tree
tree <- rpart(model_formula, data = train_set)
rpart.plot(tree, type = 2)

# DESCRIPTION: The top value is the average house price for the houses included within a 
# certain node. For example, the root node shows that in the whole train data, the average
# cost for a house is 401,000; the root's left child node instead presents an average cost
# of 355,000 (for houses that have a floor area < 3460 sq feet). The values will vary if the seed is changed.
# The lower value represents the percentage of the whole data which is included at that node 
# (and therefore satisfies the decision criteria at every split up to that point).

# COMMENT: In classification trees there is one more value in each node, just between the two
# just described values, which is the probability. See https://www.guru99.com/r-decision-trees.html 
# for an extensive explanation of classification trees.

## Understanding the pruning mechanism with printcp()
printcp(tree)

# DESCRIPTION: The parameters included in the output of printcp() are the following:
# - CP: Complexity Parameter. A smaller cp gives a more complex model_formula, i.e., more splits.
# - nsplit: Literally it is the number of splits. A tree with nsplit = 0 is just a single leaf.
#           A tree with nsplit equal to 1 will have 2 leaves. A tree with nsplit = 2 will have 3 
#           leaves and so on. A leaf is a node that has no child nodes (terminal node).
# - rel error: This is the error on the observations used to estimate the model_formula. Multiply by baseline 
#              error to get the corresponding absolute error measure.
# - xerror: Cross validation error estimate. It is generated by the rpart built-in cross validation.
# - xstd: Standard deviation of the error.
# See https://maths-people.anu.edu.au/~johnm/courses/dm/math3346/2006/pdf/r-exs9.pdf for reference.

# DESCRIPTION: How are these parameters calculated?
# - CP: CP_n is obtained by (rel error_n - rel error_(n+1))/(nsplit_(n+1)-nsplit_n) (https://stats.stackexchange.com/questions/117908/rpart-complexity-parameter-confusion).
#   e.g., for nsplit 10:
#     (tree_cp[10, "rel error"] - tree_cp[11, "rel error"]) / (tree_cp[11, "nsplit"] - tree_cp[10, "nsplit"])
# - rel error: 1 - R^2, similar to linear regression (https://stats.stackexchange.com/questions/103018/difference-between-rel-error-and-xerror-in-rpart-regression-trees).
# - xerror: It is computed using a 10-fold cross-validation.
# - xstd: 

# As we can see, printcp() automatically prints out the optimal number of nodes basing on
# the CP (Complexity Parameter). rpart does not always deliver a pruned tree (see: 
# https://stackoverflow.com/questions/13136683/is-rpart-automatic-pruning#:~:text=No%2C%20but%20the%20defaults%20for,definition%20of%20%22early%22).)
# Sometimes, we need to prune it on our own by considering the xerror and cp. The convention says that we 
# should select the CP from the list with the smallest cross-validated error (xerror in the table).
# In other words, we shall select the lowest xerror, then prune the tree by setting the CP equal to
# the CP corresponding to such error.

# Here is a way to find out the cp related to the lowest xerror without extensively looking through the table
c_par <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
c_par

# We see that the lowest xerror is actually related to the very last split. Hence, there would be no real need 
# to prune the tree.

## Pruning the tree according to c_par
prune_tree <- prune(tree, cp = c_par)
rpart.plot(prune_tree)

# Plotting the tree with fancyRpartPlot
fancyRpartPlot(prune_tree)

# The number on the top of each node represents the branch numbers in the textual representation
# of the trees as generated by the default print() method. See:
# https://stackoverflow.com/questions/45570298/what-does-the-number-on-top-of-a-node-in-a-fancyrpartplot-decision-tree-mean.

## Showing the set of possible cost-complexity prunings of a tree from a nested set with plotcp().
# https://rdrr.io/cran/itree/man/plotcp.html
# We can compare the tree before vs after pruning
plotcp(tree)
plotcp(prune_tree)

# DESCRIPTION: What is the dotted line in plotcp()?
# The function plots the cp values against their related xerror, with xstd included.
# The dotted line represents the highest cross-validated error less than the minimum cross-validated
# error plus 1 standard deviation of the error at that tree. See
# https://stackoverflow.com/questions/21698540/whats-the-meaning-of-plotcp-result-in-rpart for reference.

## Producing predictions
# Predicting the outcome variable on the test set for the ols, tree and pruned tree models
purchase_predicted_ols <- predict(ols, test_set)
purchase_predicted_tree <- predict(tree, test_set)
purchase_predicted_prune <- predict(prune_tree, test_set)

## Reporting prediction accuracy using Root Mean Squared Error (RMSE)
# In-sample trained model_formula RMSE, comparison of ols vs Pruned Tree:
rmse_is_ols <- round(sqrt(mean(residuals(ols)^2)), 4)
rmse_is_tree <- round(sqrt(mean(residuals(tree)^2)), 4)
rmse_is_prune <- round(sqrt(mean(residuals(prune_tree)^2)), 4)

rmse_is_ols
rmse_is_tree
rmse_is_prune

# Creating a function to calculate the RMSE out of sample
rmse_oos <- function(actuals, preds) {
  sqrt(mean((preds - actuals)^2))
}

# Out-of-sample test data RMSE:
rmse_oos_ols <- rmse_oos(purchase_predicted_ols, test_set[, dv])
rmse_oos_tree <- rmse_oos(purchase_predicted_tree, test_set[, dv])
rmse_oos_prune <- rmse_oos(purchase_predicted_prune, test_set[, dv])

rmse_oos_ols
rmse_oos_tree
rmse_oos_prune

## Bootstrapping the split-sample CV RMSEs to see if results are consistent
sample_boot <- function(dataset, model_formula, yvar) {
  # Splitting the dataset in train and test sets
  train_indices <- sample(1:nrow(fulldata), size = 0.75 * nrow(fulldata))
  train_set <- fulldata[train_indices, ]
  test_set <- fulldata[-train_indices, ]
  
  # Building the tree
  tree <- rpart(model_formula, data = train_set)
  
  # Finding the lowest cp, pruning the tree
  c_par <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
  prune_tree <- prune(tree, cp = c_par)
  
  # Making predictions
  purchase_predicted_ols <- predict(ols, test_set)
  purchase_predicted_tree <- predict(tree, test_set)
  purchase_predicted_prune <- predict(prune_tree, test_set)
  
  # Calculating in-sample errors
  rmse_is_ols <- round(sqrt(mean(residuals(ols)^2)), 4)
  rmse_is_tree <- round(sqrt(mean(residuals(tree)^2)), 4)
  rmse_is_prune <- round(sqrt(mean(residuals(prune_tree)^2)), 4)
  
  # Calculating out-of-sample errors
  rmse_oos_ols <- rmse_oos(purchase_predicted_ols, test_set[, yvar])
  rmse_oos_tree <- rmse_oos(purchase_predicted_tree, test_set[, yvar])
  rmse_oos_prune <- rmse_oos(purchase_predicted_prune, test_set[, yvar])
  
  # Storing the errors in a dataframe
  error_comparison <- c(rmse_is_ols, rmse_oos_ols, rmse_is_tree, rmse_oos_tree,
                                 rmse_is_prune, rmse_oos_prune)
}

set.seed(2012)
boot_rmse <- replicate(100, sample_boot(fulldata, model_formula, dv))
 
boot_rmse_is_ols <- mean(boot_rmse[1, ])
boot_rmse_oos_ols <- mean(boot_rmse[2, ])
boot_rmse_is_tree <- mean(boot_rmse[3, ])
boot_rmse_oos_tree <- mean(boot_rmse[4, ])
boot_rmse_is_prune <- mean(boot_rmse[5, ])
boot_rmse_oos_prune <- mean(boot_rmse[6, ])

comparison <- matrix(c(boot_rmse_is_ols, boot_rmse_oos_ols, boot_rmse_is_tree, boot_rmse_oos_tree, 
                       boot_rmse_is_prune, boot_rmse_oos_prune), ncol = 3, byrow = FALSE)

colnames(comparison) <- c("rmse_ols", "rmse_tree", "rmse_prune")
rownames(comparison) <- c("is", "oos")

comparison

## Implementing LOOCV
fold_i_pe <- function(i, k, estimated_model, dataset, outcome) {
  folds <- cut(1:nrow(dataset), breaks=k, labels=FALSE)
  test_indices <- which(folds==i)
  test_set <- dataset[test_indices, ]
  train_set <- dataset[-test_indices, ]
  trained_model <- update(estimated_model, data = train_set)
  predictions <- predict(trained_model, test_set)
  dataset[test_indices, outcome] - predictions # predictive error
}

k_fold_rmse <- function(estimated_model, dataset, outcome, k=10) {
  shuffled_indicies <- sample(1:nrow(dataset))
  dataset <- dataset[shuffled_indicies,]

  fold_pred_errors <- sapply(1:k, \(kth) {
    fold_i_pe(kth, k, estimated_model, dataset, outcome)
  })
  
  pred_errors <- unlist(fold_pred_errors)
  rmse <- \(errs) sqrt(mean(errs^2))
  c(rmse_is = rmse(residuals(estimated_model)), rmse_oos = rmse(pred_errors))
}

k_fold_rmse_ols <- k_fold_rmse(ols, fulldata, dv, k = 50)
k_fold_rmse_tree <- k_fold_rmse(tree, fulldata, dv, k = 50)
k_fold_rmse_prune <- k_fold_rmse(prune_tree, fulldata, dv, k = 50)

k_fold_rmse_ols # still oos < is ...
k_fold_rmse_tree
k_fold_rmse_prune

## Implementing Bagging
# Creating the functions for learning and predicting
bagged_learn <- function(estimated_model, dataset, b=100) {
  lapply(1:b, \(i) {
    data_i <- dataset[sample(nrow(dataset), replace=TRUE),]
    update(estimated_model, data=data_i) 
  })
}

bagged_predict <- function(bagged_models, new_data) {
  predictions <- lapply(bagged_models, \(m) predict(m, new_data))
  as.data.frame(predictions) |> apply(FUN=mean, MARGIN=1)
}

# Computing the RMSEs between ols and prune_tree bagged models
rmse_bag_ols <- bagged_learn(estimated_model = ols, dataset = train_set) |>
  bagged_predict(new_data = test_set) |> 
  rmse_oos(actuals = test_set[, dv])

rmse_bag_prune <- bagged_learn(estimated_model = prune_tree, dataset = train_set) |>
  bagged_predict(new_data = test_set) |> 
  rmse_oos(actuals = test_set[, dv])

rmse_bag_ols
rmse_bag_prune

## Implementing Boosting
# Creating the functions for learning and predicting
boost_learn <- function(estimated_model, dataset, outcome, n=100, rate=0.1) { 
  predictors <- dataset[,-which(names(dataset) == outcome)]
  
  res <- dataset[,outcome]
  models <- list()
  
  for (i in 1:n) {
    new_data <- cbind(res, predictors)
    colnames(new_data)[1] = outcome
    this_model <- update(estimated_model, data = new_data)
    res <- res - rate * predict(this_model, dataset)
    models[[i]] <- this_model
  }
  
  list(models=models, rate=rate)
}
  
boost_predict <- function(boosted_learning, new_data) {
  boosted_models <- boosted_learning$models
  rate <- boosted_learning$rate
  predictions <- lapply(boosted_models, \(this_model) {
    predict(this_model, new_data)
  })
  pred_frame <- as.data.frame(predictions) |> unname()
  apply(pred_frame, FUN = \(preds) rate * sum(preds), MARGIN=1)
}
  
# Comparing the RMSEs between ols and prune_tree boosted models
rmse_boost_ols <- boost_learn(ols, train_set, dv) |>
  boost_predict(test_set) |> rmse_oos(actuals = test_set[, dv])

rmse_boost_prune <- boost_learn(prune_tree, train_set, dv) |>
  boost_predict(test_set) |> rmse_oos(actuals = test_set[, dv])

rmse_boost_ols
rmse_boost_prune

# Final comparisons
error_comparison <- matrix(c(rmse_is_ols, rmse_oos_ols, rmse_is_prune, rmse_oos_prune, k_fold_rmse_ols[1], 
                       k_fold_rmse_ols[2], k_fold_rmse_prune[1], k_fold_rmse_prune[2]), ncol = 4, byrow = FALSE)

colnames(error_comparison) <- c("rmse_ols", "rmse_prune", "k_fold_rmse_ols", "k_fold_rmse_prune")
rownames(error_comparison) <- c("is", "oos")

error_comparison

bag_boost_comparison <- matrix(c(rmse_bag_ols, rmse_boost_ols, rmse_bag_prune, rmse_boost_prune), 
                               ncol = 2, byrow = FALSE)

colnames(bag_boost_comparison) <- c("rmse_ols", "rmse_prune")
rownames(bag_boost_comparison) <- c("bag", "boost")

bag_boost_comparison
